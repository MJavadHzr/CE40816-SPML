{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ooJtNr5dGrH9"
   },
   "source": [
    "**Name:** Javad Hezareh\n",
    "\n",
    "**Student Number:** 98101074\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJm9Z1k0cdmh"
   },
   "source": [
    "# Neural-Network with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDN075MYGesD"
   },
   "source": [
    "In this notebook, you are going to write and implement all the components required to create and train a two-layered neural network using NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wt3FdxgNcdmm"
   },
   "source": [
    "## Imports & Seeding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPZ4zlnxqhl5"
   },
   "source": [
    "Importing some common libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Et7OS7TGcdmn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(123)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fa2v2-xbcdmo"
   },
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKWqV2Gycdmp"
   },
   "source": [
    "You'll train and evaluate your model on [Fashion MNIST](https://en.wikipedia.org/wiki/Fashion_MNIST) dataset. In this section, you'll download Fashion MNIST and split it into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tMYZtSoLc7c-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Using `fetch_openml`, download `Fashion-MNIST` \n",
    "# and save the training data and labels in `X` and `y` respectively.\n",
    "#############################\n",
    "# Your code goes here (5 points)\n",
    "X, y = fetch_openml('Fashion-MNIST', version=1, return_X_y=True, cache=True)\n",
    "#############################\n",
    "\n",
    "# Normalization:\n",
    "X = ((X / 255.) - .5) * 2\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sDmxyMJ4dBk3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using `train_test_split`, split your data into two sets. \n",
    "# Set the test_size to 10000\n",
    "\n",
    "#############################\n",
    "# Your code goes here (6 points)\n",
    "y = y.astype(np.int32)#.to_numpy()\n",
    "X = X.to_numpy()\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=10000)\n",
    "#############################\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiGTXGXKcdmt"
   },
   "source": [
    "## Prepare training & validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba3nNYlDcdmt"
   },
   "source": [
    "We'll use only 3 classes from Fashion MNIST: Trouser, T-shirt, and Sneaker classes.\n",
    "\n",
    "The class labels for T-shirt, Trouser, and Sneaker are 0, 1, and 7 respectively.\n",
    "\n",
    "In this part, you'll limit the testing and training sets to only these three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TcBDZEtzcdmu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18017, 784) (18017,)\n"
     ]
    }
   ],
   "source": [
    "# Modify `y_train` and `x_train`.\n",
    "# Only keep the 3 classes mentioned above. \n",
    "#############################\n",
    "# Your code goes here (4 points)\n",
    "x_train = x_train[np.isin(y_train, [0, 1, 7])]\n",
    "y_train = y_train[np.isin(y_train, [0, 1, 7])]\n",
    "#############################\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LX2hkRe1cdmw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2983, 784) (2983,)\n"
     ]
    }
   ],
   "source": [
    "# Modify `y_test` and `x_test`.\n",
    "# Only keep the 3 classes mentioned above. \n",
    "#############################\n",
    "# Your code goes here (4 points)\n",
    "x_test = x_test[np.isin(y_test, [0, 1, 7])]\n",
    "y_test = y_test[np.isin(y_test, [0, 1, 7])]\n",
    "#############################\n",
    "\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv6SMLUktWbv"
   },
   "source": [
    "## Linear & Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXlyJo5JteKC"
   },
   "source": [
    "In this part, you'll implement the forward and backward process for the following components:\n",
    "- Softmax Layer\n",
    "- Linear Layer\n",
    "- ReLU Layer\n",
    "- Sigmoid Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXtAD5uYA4sQ"
   },
   "source": [
    "### The `Softmax` Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tzaIVo-_Axp7"
   },
   "outputs": [],
   "source": [
    "class SoftMaxLayer(object):\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # Write the forward pass for softmax.\n",
    "        # Save the values required for the backward pass.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        exps = np.exp(inp)\n",
    "        self.output = exps / exps.sum(axis=1)[:, None]\n",
    "        #############################\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, up_grad):\n",
    "        # Write the backward pass for softmax.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        grad = (self.output * up_grad) - (self.output * (up_grad * self.output).sum(axis=1)[:, None])\n",
    "        #############################\n",
    "        return grad\n",
    "\n",
    "    def step(self, optimizer):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcFoIDZjcdnB"
   },
   "source": [
    "### The `Linear` Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1strsTh6cdnG"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        # Initialize the layer's weights and biases\n",
    "        #############################\n",
    "        # Your code goes here (2 points)\n",
    "        self.w = np.random.normal(size=(in_dim, out_dim), scale=np.sqrt(2/in_dim))\n",
    "        self.b = np.zeros(out_dim)\n",
    "        #############################\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        # Compute linear layer's output.\n",
    "        # Save the value(s) required for the backward phase.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        self.inp = inp\n",
    "        z = inp @ self.w + self.b[None, :]\n",
    "        #############################\n",
    "        return z\n",
    "    \n",
    "    def backward(self, up_grad):\n",
    "        # Calculate the gradient with respect to the weights \n",
    "        # and biases and save the results.\n",
    "        #############################\n",
    "        # Your code goes here (6 points)\n",
    "        self.db = up_grad.sum(axis=0)\n",
    "        self.dw = self.inp.T @ up_grad\n",
    "        self.dx = up_grad @ self.w.T\n",
    "        down_grad = self.dx\n",
    "        #############################\n",
    "        return down_grad\n",
    "\n",
    "    def step(self, optimizer):\n",
    "        # Update the layer's weights and biases\n",
    "        # Update previous_w_update and previous_b_update accordingly\n",
    "        #############################\n",
    "        # Your code goes here (5 points)\n",
    "        self.w = optimizer.get_next_update(self.w, self.dw)\n",
    "        self.b = optimizer.get_next_update(self.b, self.db)\n",
    "        #############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0Lfo-nhcdnG"
   },
   "source": [
    "### The `ReLU` Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tN6vcirMcdnH"
   },
   "outputs": [],
   "source": [
    "class RelU:\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # Write the forward pass for ReLU.\n",
    "        # Save the value(s) required for the backward pass.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        self.inp = inp\n",
    "        output = inp * (inp > 0)\n",
    "        #############################\n",
    "        return output\n",
    "    \n",
    "    def backward(self, up_grad):\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        down_grad = up_grad * (self.inp > 0)\n",
    "        #############################\n",
    "        return down_grad\n",
    "\n",
    "    def step(self, optimizer):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z00KoSI3cdnJ"
   },
   "source": [
    "### The `sigmoid` Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TTYYeL2lcdnJ"
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        self.inp = inp\n",
    "\n",
    "        y = np.where(inp >= 0, 1 / (1 + np.exp(-inp)), np.exp(inp) / (1 + np.exp(inp)))\n",
    "        self.output = y\n",
    "        #############################\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, up_grad):\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        sigmoid = self.output\n",
    "        down_grad = sigmoid * (1 - sigmoid) * up_grad\n",
    "        #############################\n",
    "        return down_grad\n",
    "    \n",
    "    def step(self, optimizer):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zngleGY2cdnK"
   },
   "source": [
    "## `Loss` function :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISedT4FvcdnK"
   },
   "source": [
    "For this task we are going to use the [Cross-Entropy Loss](https://en.wikipedia.org/wiki/Cross_entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XQyz4ybycdnL"
   },
   "outputs": [],
   "source": [
    "class CELoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "\n",
    "        self.yhat = pred\n",
    "        self.y = target\n",
    "        m = self.y.shape[0]\n",
    "        # Commpute and return the loss\n",
    "        #############################\n",
    "        # Your code goes here (8 points)\n",
    "        loss = - np.log(np.where(target != 0, pred, 1)).sum()\n",
    "        return loss / m\n",
    "        #############################\n",
    "\n",
    "    def backward(self):\n",
    "        # Derivative of loss_fn with respect to a the predicted label.\n",
    "        # Use `self.y` and `self.yhat` to compute and return `grad`.\n",
    "        #############################\n",
    "        # Your code goes here (6 points)\n",
    "        grad = - (1 / self.yhat) * self.y / self.y.shape[0]\n",
    "        #############################\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xovZI-70kB9I"
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "In this section, you'll implement an optimizer classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "h5ADTi5tkVTS"
   },
   "outputs": [],
   "source": [
    "class GradientDescent(object):\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def get_next_update(self, x, dx):\n",
    "        # Compute the new value for 'x' and return the result\n",
    "        #############################\n",
    "        # Your code goes here (2 points)\n",
    "        new_x = x - self.lr * dx\n",
    "        #############################\n",
    "        return new_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxxrEEovYEFi"
   },
   "source": [
    "## The Model\n",
    "Now you'll write the base class for a multi-layer perceptron network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "t8SoZeYRcdnY"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layers, loss_fn, optimizer):\n",
    "        self.layers = layers \n",
    "        self.losses  = [] \n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # Pass `inp` to all the layers sequentially\n",
    "        # and return the result.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        out = inp\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        #############################\n",
    "        return out\n",
    "        \n",
    "    def loss(self, pred, label):\n",
    "        assert pred.shape == label.shape\n",
    "        \n",
    "        loss = self.loss_fn.forward(pred, label)\n",
    "        self.losses.append(loss)\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        # Start with loss function's gradient and \n",
    "        # do the backward pass on all the layers.\n",
    "        #############################\n",
    "        # Your code goes here (5 points)\n",
    "        up_grad = self.loss_fn.backward()\n",
    "        for layer in self.layers[::-1]:\n",
    "            up_grad = layer.backward(up_grad)\n",
    "        #############################\n",
    "        \n",
    "    def update(self):\n",
    "        for layer in self.layers:\n",
    "          layer.step(self.optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zo0rNwYciueF"
   },
   "source": [
    "The following cell encodes training labels into a one-hot representation with 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nhJTulaFJ4vR"
   },
   "outputs": [],
   "source": [
    "def onehot_enc(y, num_labels):\n",
    "    # ary = np.zeros((y.shape[0], num_labels))\n",
    "    # for i, val in enumerate(y):\n",
    "    #     ary[i, val] = 1\n",
    "    # return ary\n",
    "    ones = np.eye(8)\n",
    "    return ones[y][:, [0, 1, 7]]\n",
    "\n",
    "y_train_one = onehot_enc(y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TS6S_RUwsRkF"
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, x, y):\n",
    "    for n in range(epochs):\n",
    "      # First do the forward pass. Next, compute the loss.\n",
    "      # Then do the backward pass and finally, update the parameters.\n",
    "      #############################\n",
    "      # Your code goes here (4 points)\n",
    "      probs = model.forward(x)\n",
    "      loss = model.loss(probs, y)\n",
    "      model.backward()\n",
    "      model.update()\n",
    "      #############################\n",
    "      print(f\"Loss at {n}: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "m1lSq2jNcdnY",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 0: 1.135\n",
      "Loss at 1: 1.115\n",
      "Loss at 2: 1.095\n",
      "Loss at 3: 1.076\n",
      "Loss at 4: 1.057\n",
      "Loss at 5: 1.038\n",
      "Loss at 6: 1.021\n",
      "Loss at 7: 1.005\n",
      "Loss at 8: 0.989\n",
      "Loss at 9: 0.975\n",
      "Loss at 10: 0.962\n",
      "Loss at 11: 0.950\n",
      "Loss at 12: 0.939\n",
      "Loss at 13: 0.929\n",
      "Loss at 14: 0.919\n",
      "Loss at 15: 0.910\n",
      "Loss at 16: 0.901\n",
      "Loss at 17: 0.893\n",
      "Loss at 18: 0.886\n",
      "Loss at 19: 0.878\n",
      "Loss at 20: 0.872\n",
      "Loss at 21: 0.865\n",
      "Loss at 22: 0.859\n",
      "Loss at 23: 0.853\n",
      "Loss at 24: 0.847\n",
      "Loss at 25: 0.842\n",
      "Loss at 26: 0.837\n",
      "Loss at 27: 0.832\n",
      "Loss at 28: 0.827\n",
      "Loss at 29: 0.822\n",
      "Loss at 30: 0.818\n",
      "Loss at 31: 0.813\n",
      "Loss at 32: 0.809\n",
      "Loss at 33: 0.805\n",
      "Loss at 34: 0.801\n",
      "Loss at 35: 0.797\n",
      "Loss at 36: 0.793\n",
      "Loss at 37: 0.790\n",
      "Loss at 38: 0.786\n",
      "Loss at 39: 0.783\n",
      "Loss at 40: 0.779\n",
      "Loss at 41: 0.776\n",
      "Loss at 42: 0.773\n",
      "Loss at 43: 0.770\n",
      "Loss at 44: 0.767\n",
      "Loss at 45: 0.764\n",
      "Loss at 46: 0.761\n",
      "Loss at 47: 0.758\n",
      "Loss at 48: 0.756\n",
      "Loss at 49: 0.753\n",
      "Loss at 50: 0.751\n",
      "Loss at 51: 0.748\n",
      "Loss at 52: 0.746\n",
      "Loss at 53: 0.743\n",
      "Loss at 54: 0.741\n",
      "Loss at 55: 0.739\n",
      "Loss at 56: 0.736\n",
      "Loss at 57: 0.734\n",
      "Loss at 58: 0.732\n",
      "Loss at 59: 0.730\n",
      "Loss at 60: 0.728\n",
      "Loss at 61: 0.726\n",
      "Loss at 62: 0.724\n",
      "Loss at 63: 0.722\n",
      "Loss at 64: 0.721\n",
      "Loss at 65: 0.719\n",
      "Loss at 66: 0.717\n",
      "Loss at 67: 0.715\n",
      "Loss at 68: 0.714\n",
      "Loss at 69: 0.712\n",
      "Loss at 70: 0.710\n",
      "Loss at 71: 0.709\n",
      "Loss at 72: 0.707\n",
      "Loss at 73: 0.706\n",
      "Loss at 74: 0.704\n",
      "Loss at 75: 0.703\n",
      "Loss at 76: 0.701\n",
      "Loss at 77: 0.700\n",
      "Loss at 78: 0.698\n",
      "Loss at 79: 0.697\n",
      "Loss at 80: 0.696\n",
      "Loss at 81: 0.694\n",
      "Loss at 82: 0.693\n",
      "Loss at 83: 0.692\n",
      "Loss at 84: 0.691\n",
      "Loss at 85: 0.689\n",
      "Loss at 86: 0.688\n",
      "Loss at 87: 0.687\n",
      "Loss at 88: 0.686\n",
      "Loss at 89: 0.685\n",
      "Loss at 90: 0.684\n",
      "Loss at 91: 0.683\n",
      "Loss at 92: 0.682\n",
      "Loss at 93: 0.680\n",
      "Loss at 94: 0.679\n",
      "Loss at 95: 0.678\n",
      "Loss at 96: 0.677\n",
      "Loss at 97: 0.676\n",
      "Loss at 98: 0.675\n",
      "Loss at 99: 0.675\n",
      "Loss at 100: 0.674\n",
      "Loss at 101: 0.673\n",
      "Loss at 102: 0.672\n",
      "Loss at 103: 0.671\n",
      "Loss at 104: 0.670\n",
      "Loss at 105: 0.669\n",
      "Loss at 106: 0.668\n",
      "Loss at 107: 0.667\n",
      "Loss at 108: 0.667\n",
      "Loss at 109: 0.666\n",
      "Loss at 110: 0.665\n",
      "Loss at 111: 0.664\n",
      "Loss at 112: 0.663\n",
      "Loss at 113: 0.663\n",
      "Loss at 114: 0.662\n",
      "Loss at 115: 0.661\n",
      "Loss at 116: 0.660\n",
      "Loss at 117: 0.660\n",
      "Loss at 118: 0.659\n",
      "Loss at 119: 0.658\n",
      "Loss at 120: 0.658\n",
      "Loss at 121: 0.657\n",
      "Loss at 122: 0.656\n",
      "Loss at 123: 0.656\n",
      "Loss at 124: 0.655\n",
      "Loss at 125: 0.654\n",
      "Loss at 126: 0.654\n",
      "Loss at 127: 0.653\n",
      "Loss at 128: 0.653\n",
      "Loss at 129: 0.652\n",
      "Loss at 130: 0.651\n",
      "Loss at 131: 0.651\n",
      "Loss at 132: 0.650\n",
      "Loss at 133: 0.650\n",
      "Loss at 134: 0.649\n",
      "Loss at 135: 0.648\n",
      "Loss at 136: 0.648\n",
      "Loss at 137: 0.647\n",
      "Loss at 138: 0.647\n",
      "Loss at 139: 0.646\n",
      "Loss at 140: 0.646\n",
      "Loss at 141: 0.645\n",
      "Loss at 142: 0.645\n",
      "Loss at 143: 0.644\n",
      "Loss at 144: 0.644\n",
      "Loss at 145: 0.643\n",
      "Loss at 146: 0.643\n",
      "Loss at 147: 0.642\n",
      "Loss at 148: 0.642\n",
      "Loss at 149: 0.641\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the `MLP` with the following structure:\n",
    "#     Linear with 50 units --> ReLU --> Linear with 50 units --> ReLU --> Linear with 3 units --> Sigmoid --> Softmax\n",
    "# Use GradientDescent as the optimizer, set the learning rate to 0.001, and use CELoss as the loss function.\n",
    "#############################\n",
    "# Your code goes here (4 points)\n",
    "in_dim = x_train.shape[1]\n",
    "layers = [\n",
    "    Linear(in_dim, 50),\n",
    "    RelU(),\n",
    "    Linear(50, 50),\n",
    "    RelU(),\n",
    "    Linear(50, 3),\n",
    "    Sigmoid(),\n",
    "    SoftMaxLayer()\n",
    "]\n",
    "\n",
    "optimizer = GradientDescent(0.01)\n",
    "loss_fn = CELoss()\n",
    "nn = MLP(layers, loss_fn, optimizer)\n",
    "#############################\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "# Train the network using only `x_train` and `y_train` (no validation)\n",
    "train(nn, epochs, x_train, y_train_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJec2xRJmY37"
   },
   "source": [
    "Let's plot the loss value for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ymaQNn70cdnZ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses)\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dz3yqRa1cdna"
   },
   "source": [
    "**Let's also check our model's performance using the `accuracy` metric on the `testing` dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TRqwXho7cdnd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9708347301374455\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy on the testing set\n",
    "#############################\n",
    "# Your code goes here (7 points)\n",
    "probs = nn.forward(x_test)\n",
    "preds = np.argmax(probs, axis=1)\n",
    "\n",
    "y_test_map = np.where(y_test == 7, 2, y_test)\n",
    "acc = np.average(preds == y_test_map)\n",
    "#############################\n",
    "\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
